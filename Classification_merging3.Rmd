---
title: "Project"
author: "Guanghua Qiao"
date: "2020.6.13"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#load('D:/SJTU Lessons/GSE137140.Rdata')
# pd: clinical
# exp
load("D:/SJTU Lessons/Rawdata1.Rdata")
# Cancerexp,Noncancerexp,Cancer,Noncancer
can=cbind(Cancer,t(Cancerexp))
noncan=cbind(Noncancer,t(Noncancerexp))
table(can$stage)
table(can$type)
# Groups need to be merged
x=can[,-c(1,4:8)]
y=noncan[,-c(1:3)]
y$stage='0'
y$type='0'
# merging & deleting
#x1=x[-which(x$stage=='IV'),]
x2=x
x2$stage[which(x$stage %in% c('IIIA','IIIB','IV'))]='III'
x2$stage[which(x$stage %in% c('IIA','IIB'))]='II'
x2$stage[which(x$stage %in% c('IA','IB'))]='I'
# age & sex
str(y$Age)
y1=y
for (i in 1:dim(y1)[1]){
  y1$Age[i]=as.numeric(unlist(strsplit(y$Age[i],split=': '))[2])
}
y2=na.omit(y1)
y2$Age=as.numeric(y2$Age)
y3=y2[-which(y2$Age==7),]
x3=x2
for (i in 1:dim(x2)[1]){
  x3$Age[i]=as.numeric(unlist(strsplit(x2$Age[i],split=': '))[2])
}
x3$Age=as.numeric(x3$Age)

rawcombine=rbind(x3,y3)
str(rawcombine$Age)

table(x3$Sex)
summary(x3$Age)
summary(y3$Age)
ggplot(rbind(x3,y3), aes(x=stage,y=Age))+geom_boxplot()

######  seperate train & test set  ######
# extract train set directly
age.size=c(sum(y3$Age<30),
sum(y3$Age>=30 & y3$Age<40),
sum(y3$Age>=40 & y3$Age<50),
sum(y3$Age>=50 & y3$Age<60),
sum(y3$Age>=60 & y3$Age<70),
sum(y3$Age>=70 & y3$Age<80),
sum(y3$Age>=80))
strata.size=age.size*100/2176
strata.size2=c(1,18,28,30,17,5,1)
y.str=y3
for (i in 1:dim(y.str)[1]){
  if (y.str$Age[i]<30) {
    y.str$AgeGroup[i]='age20s'
  } else if (y.str$Age[i]<40){
    y.str$AgeGroup[i]='age30s'
  } else if (y.str$Age[i]<50){
    y.str$AgeGroup[i]='age40s'
  } else if (y.str$Age[i]<60){
    y.str$AgeGroup[i]='age50s'
  } else if (y.str$Age[i]<70){
    y.str$AgeGroup[i]='age60s'
  } else if (y.str$Age[i]<80){
    y.str$AgeGroup[i]='age70s'
  } else{
    y.str$AgeGroup[i]='age80s'
  }
}
y.str$AgeGroup=as.factor(y.str$AgeGroup)
y.str=y.str[order(y.str$AgeGroup),]
library(sampling)
set.seed(11)
strata.train=strata(y.str,stratanames=("AgeGroup"),size=strata.size2,method="srswor")
noncan.train=y.str[strata.train$ID_unit,]


x.str=x3
for (i in 1:dim(x.str)[1]){
  if (x.str$Age[i]<40) {
    x.str$AgeGroup[i]='age30s'
  } else if (x.str$Age[i]<50){
    x.str$AgeGroup[i]='age40s'
  } else if (x.str$Age[i]<60){
    x.str$AgeGroup[i]='age50s'
  } else if (x.str$Age[i]<70){
    x.str$AgeGroup[i]='age60s'
  } else if (x.str$Age[i]<80){
    x.str$AgeGroup[i]='age70s'
  } else{
    x.str$AgeGroup[i]='age80s'
  }
}
x.str$AgeGroup=as.factor(x.str$AgeGroup)
x.str=x.str[order(x.str$stage,x.str$AgeGroup),]
(strata.size_1=table(x.str$stage,x.str$AgeGroup)[1,]*100/971)
(strata.size_2=table(x.str$stage,x.str$AgeGroup)[2,]*100/207)
(strata.size_3=table(x.str$stage,x.str$AgeGroup)[3,]*100/178)
# 3*6=18
strata.size3=c(1,5,17,32,39,6,2,5,12,40,32,9,2,12,14,38,29,5)

set.seed(11)
strata.train2=strata(x.str,stratanames=c("stage","AgeGroup"),size=strata.size3,method="srswor")
can.train=x.str[strata.train2$ID_unit,]


table(can.train$stage)
train=rbind(can.train,noncan.train)
test.all=rbind(y.str[-strata.train$ID_unit,],x.str[-strata.train2$ID_unit,])
train$stage=as.factor(train$stage)
test.all$stage=as.factor(test.all$stage)
train=train[,-dim(train)[2]]
test.all=test.all[,-dim(test.all)[2]]
test.all=test.all[,-dim(test.all)[2]]

ggplot(train, aes(x=stage,y=Age))+geom_boxplot()
ggplot(test.all, aes(x=stage,y=Age))+geom_boxplot()
table(test.all$stage)


######  pre-process  ######
# normalization?
# miRNA294
library(ggplot2)
library(tidyr)
library(reshape2)
mydata<-melt(train[,c(1:3,297:299)],                       
       variable.name="miRNA",         
       value.name="value"          
       )
ggplot(mydata, aes(x=miRNA,y=value))+geom_boxplot()

#dim(train)
#which(c('miR-4463','miR-2861','miR-1493-p') %in% colnames(train))
#which('hsa-miR-1493-p' %in% colnames(train))


######  LDA & QDA  ######
library(MASS)
library(pROC)
lda.fit = lda(stage ~ ., data=train[,-c(2:4)])
#lda.fit
#plot(lda.fit)
#names(lda.fit)
lda.pred = predict(lda.fit, test.all)
table(lda.pred$class, test.all$stage)
mean(lda.pred$class == test.all$stage)

lda.fit=lda(stage~miRNA294+miRNA635,data=train.tree)
lda.pred = predict(lda.fit, test.tree)
table(lda.pred$class, test.tree$stage)
mean(lda.pred$class == test.tree$stage)

macro.ave(as.matrix(table(lda.pred$class, test.all$stage)))
can_or_non(as.matrix(table(lda.pred$class, test.all$stage)))

correct.rate.max=0
correctrate=rep(NA,641)
auc=rep(NA,641)
for (i in 1:641){
  if (i==294){
    correctrate[i]=NA
    auc[i]=NA
  }else{
    lda.fit=lda(stage~miRNA294+eval(as.symbol(paste0('miRNA',i))),data=train.tree)
    lda.pred = predict(lda.fit, test.tree)
    roc=multiclass.roc(ordered(test.tree$stage),ordered(lda.pred$class))
    correct.rate=mean(lda.pred$class == test.tree$stage)
    if (correct.rate>correct.rate.max) {
      correct.rate.max=correct.rate
      k <<- i
    }
    correctrate[i]=correct.rate
    auc[i]=roc$auc
  }
}
correct.rate.max
k
max(auc)
which(auc==max(auc))
correctrate[189]
auc[635]

# Attention! change of length: 641 -> 640
correctrate=na.omit(correctrate)
auc=na.omit(auc)
#plot(1:640,correctrate,type='l',col='green',xlab='Two-miRNA panels',ylab='Correct rate & AUC',ylim=c(0.5,.75))
#lines(1:640,auc,col='red')
#legend("bottomright", c('Correct rate', "AUC"), col=c("green", "red"), cex=1, lty=1)

colnames(train[,-c(1:4)])[c(294,215,632,244,302)]
colnames(train[,-c(1:4)])[c(294,635,189)]


######  KNN  ######
library(class)
library(pROC)
train.X = as.matrix(train[,-c(1:4)])
test.X = as.matrix(test.all[,-c(1:2,dim(test.all)[2])])

set.seed(114)
knn.pred = knn(train.X, test.X, train$stage, k=112)
table(knn.pred, test.all$stage)
mean(knn.pred == test.all$stage)
roc=multiclass.roc(ordered(test.all$stage),ordered(knn.pred))
#str(knn.pred)
#str(ordered(knn.pred))

correct.rate.max=0
correctrate=rep(NA,150)
auc=rep(NA,150)
## may add macro-ave in this 
for (i in 1:150){
  set.seed(114)
  knn.pred = knn(train.X, test.X, train$stage, k=i)
  roc=multiclass.roc(ordered(test.all$stage),ordered(knn.pred))
  correct.rate=mean(knn.pred == test.all$stage)
  if (correct.rate>correct.rate.max) {
    correct.rate.max=correct.rate
    k <<- i
  }
  correctrate[i]=correct.rate
  auc[i]=roc$auc
}
correct.rate.max
k
auc[112]
#correct.rate=0.6961,k=92
# k=92 is not suitable
# maybe conclude that KNN perform badly, predicting only categories of 0 & IA (for KNN not suitable for high-D data).
# k=50 only predicts 0, IA & IB

plot(1:150,correctrate,type='l',col='green',xlab='K',ylab='Correct rate & AUC',ylim=c(0.5,.75))
lines(1:150,auc,col='red')
legend("bottomright", c('Correct rate', "AUC"), col=c("green", "red"), cex=1, lty=1)

max(auc)
which(auc==max(auc))
correctrate[45]


######  tree  ######
library(MASS)
library(tree)
library(randomForest)

train.tree <- train[,-c(2:4)]
colnames(train.tree)=c("stage", paste0("miRNA", 1:641))
test.tree <- test.all[,-c(1:2)]
colnames(test.tree)=c(paste0("miRNA", 1:641),"stage")
#train.tree$stage=as.factor(train.tree$stage)
#test.tree$stage=as.factor(test.tree$stage)

set.seed(98)
tree.Cancer = tree(stage ~ .,train.tree)
summary(tree.Cancer)
tree.Cancer
plot(tree.Cancer)
text(tree.Cancer, pretty = 0)
pred = predict(tree.Cancer, test.tree, type = "class")
table(pred,test.tree$stage)

set.seed(1341)
cv.Cancer= cv.tree(tree.Cancer, FUN = prune.misclass)
#cv.Cancer= cv.tree(tree.Cancer, FUN = prune.tree)
cv.Cancer
plot(cv.Cancer$size, cv.Cancer$dev, type = "b", xlab = "Tree Size", ylab = "CV misclass rate")
msize = cv.Cancer$size[which.min(cv.Cancer$dev)]
Cancer.pruned = prune.misclass(tree.Cancer, best = msize)
Cancer.pruned
summary(Cancer.pruned)
plot(Cancer.pruned)
text(Cancer.pruned, pretty=0)

misclass.unpruned = sum(test.tree$stage != pred)
misclass.unpruned / length(pred)
pred.pruned = predict(Cancer.pruned, test.tree, type = "class")
misclass.pruned = sum(test.tree$stage != pred.pruned)
misclass.pruned / length(pred.pruned)
table(pred.pruned,test.tree$stage)


# Random Forest
X.train = train.tree[,-1]
X.test = test.tree[,-dim(test.tree)[2]]
Y.train = train.tree[,1]
Y.test = test.tree[,dim(test.tree)[2]]
p = dim(train.tree)[2] - 1
p.2 = p / 2
p.sq = sqrt(p)
set.seed(666)
rf.p = randomForest(X.train, Y.train, xtest=X.test, ytest=Y.test, mtry=p, ntree=1000)
rf.p.2 = randomForest(X.train, Y.train, xtest=X.test, ytest=Y.test, mtry=p.2, ntree=1000)
rf.p.sq = randomForest(X.train, Y.train, xtest=X.test, ytest=Y.test, mtry=p.sq, ntree=1000)
plot(1:1000, rf.p$test$err.rate[,1], col="green", type="l", xlab="Number of Trees", ylab="Test Classification Error")
lines(1:1000, rf.p.2$test$err.rate[,1], col="red", type="l")
lines(1:1000, rf.p.sq$test$err.rate[,1], col="blue", type="l")
legend("topright", c("m=p", "m=p/2", "m=sqrt(p)"), col=c("green", "red", "blue"), cex=1, lty=1)

# run for 1 hour...
errRate <- c(1)
for (i in 1:641){
  set.seed(66)
  m <- randomForest(X.train, Y.train,mtry=i,proximity=TRUE)  
  err<-mean(m$err.rate)  
  errRate[i] <- err  
}
m=which.min(errRate)
m
plot(1:641,errRate)

set.seed(66)
rf.552 = randomForest(X.train, Y.train, xtest=X.test, ytest=Y.test, mtry=552, ntree=1000)
rf.p = randomForest(X.train, Y.train, xtest=X.test, ytest=Y.test, mtry=p, ntree=1000)
rf.p.2 = randomForest(X.train, Y.train, xtest=X.test, ytest=Y.test, mtry=p.2, ntree=1000)
plot(1:500, rf.552$test$err.rate[1:500,1], col="blue", type="l", xlab="Number of Trees", ylab="Test Classification Error")
lines(1:500, rf.p.2$test$err.rate[1:500,1], col="red", type="l")
lines(1:500, rf.p$test$err.rate[1:500,1], col="green", type="l")
legend("topright", c("m=p", "m=p/2", "m=552"), col=c("green", "red", "blue"), cex=1, lty=1)

which.min(rf.552$test$err.rate[-c(16),1])
rf.552$test$err.rate[254,1]
rf.552$test$err.rate[250,1]
plot(rf.552)
set.seed(66)
rf=randomForest(X.train, Y.train, xtest=X.test, ytest=Y.test, mtry=552, ntree=500)

rf.p$test$confusion
table(rf.p$test$predicted,Y.test)
mean(rf.p$test$predicted == Y.test)
rf.552$test$confusion
table(rf.552$test$predicted,Y.test)
mean(rf.552$test$predicted == Y.test)
rf$test$confusion
table(rf$test$predicted,Y.test)
mean(rf$test$predicted == Y.test)

head(importance(rf.p)[order(importance(rf.p),decreasing = T),])
head(importance(rf)[order(importance(rf),decreasing = T),])
colnames(train[,-c(1:4)])[c(294,10,215,302,632,67)]


# Boosting
library(gbm)
set.seed(34)
boost.Cancer = gbm(stage~., data=train.tree, n.trees=1000, shrinkage=0.01)
boost.Cancer10 = gbm(stage~., data=train.tree, n.trees=1000, shrinkage=0.01,cv.folds = 10)
boost.Cancer10
head(summary(boost.Cancer10))
boost.prob = predict(boost.Cancer10,test.tree, n.trees=1000, type="response")
stage.pred=rep(NA,dim(boost.prob)[1])
for (i in 1:dim(boost.prob)[1]){
  for (j in 1:dim(boost.prob)[2]){
    if (boost.prob[i,j,1]==max(boost.prob[i,,1])){
      stage.pred[i]=colnames(boost.prob)[j]
    }
  }
}
stage.pred=as.factor(stage.pred)
table(stage.pred,test.tree$stage)
mean(stage.pred == test.tree$stage)
colnames(train[,-c(1:4)])[c(294,215,632,244,383,302)]


# calculate macro-average for boosting model
confu=as.matrix(table(stage.pred,test.tree$stage))
macro.ave=function(confusion.matrix,test=2){
  p=dim(confusion.matrix)[2]
  if (test==2){
    y=rep(NA,p)
    for (i in 1:p){
      y[i]=confusion.matrix[i,i]/sum(confusion.matrix[,i])
    }
    return(mean(y))
  } else if (test==1){
    y=rep(NA,p)
    for (i in 1:p){
      y[i]=confusion.matrix[i,i]/sum(confusion.matrix[i,])
    }
    return(mean(y))
  }
}
macro.ave(confu)
macro.ave(as.matrix(table(lda.pred$class, test.all$stage)))
macro.ave(as.matrix(table(knn.pred, test.all$stage)))
macro.ave(as.matrix(table(pred,test.tree$stage)))
macro.ave(as.matrix(table(pred.pruned,test.tree$stage)))
macro.ave(as.matrix(table(rf.p$test$predicted,test.tree$stage)))
macro.ave(as.matrix(table(rf$test$predicted,test.tree$stage)))

can_or_non=function(confusion.matrix){
  # 4*4
  x=confusion.matrix[1,1]
  xcol=sum(confusion.matrix[,1])
  xrow=sum(confusion.matrix[1,])
  y=sum(confusion.matrix)-xcol-xrow+x
  return((x+y)/sum(confusion.matrix))
}
can_or_non(confu)
can_or_non(as.matrix(table(lda.pred$class, test.all$stage)))
can_or_non(as.matrix(table(knn.pred, test.all$stage)))
can_or_non(as.matrix(table(pred,test.tree$stage)))
can_or_non(as.matrix(table(pred.pruned,test.tree$stage)))
can_or_non(as.matrix(table(rf.p$test$predicted,test.tree$stage)))
can_or_non(as.matrix(table(rf$test$predicted,test.tree$stage)))




write.table(X.train,file="D:/SJTU Lessons/X_train.txt" , sep =" ", row.names =FALSE,col.names =F, quote =FALSE)
write.table(X.test,file="D:/SJTU Lessons/X_test.txt" , sep =" ", row.names =FALSE,col.names =F, quote =FALSE)
y_train=c()
y_test=c()
y_train[Y.train=='0']=0
y_train[Y.train=='I']=1
y_train[Y.train=='II']=2
y_train[Y.train=='III']=3
y_test[Y.test=='0']=0
y_test[Y.test=='I']=1
y_test[Y.test=='II']=2
y_test[Y.test=='III']=3
write.table(y_train,file="D:/SJTU Lessons/Y_train.txt" , sep =" ", row.names =FALSE,col.names =F, quote =FALSE)
write.table(y_test,file="D:/SJTU Lessons/Y_test.txt" , sep =" ", row.names =FALSE,col.names =F, quote =FALSE)

svm.prob=read.table("D:/SJTU Lessons/svmpred.txt",sep=' ')
colnames(svm.prob)=c('0','I','II','III')
svm.pred=rep(NA,dim(svm.prob)[1])
for (i in 1:dim(svm.prob)[1]){
  for (j in 1:dim(svm.prob)[2]){
    if (svm.prob[i,j]==max(svm.prob[i,])){
      svm.pred[i]=colnames(svm.prob)[j]
    }
  }
}
svm.pred=as.factor(svm.pred)
table(svm.pred,test.tree$stage)
mean(svm.pred == test.tree$stage)

```





