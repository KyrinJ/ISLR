---
title: "Project"
author: "Guanghua Qiao"
date: "2020.6.12"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#load('D:/SJTU Lessons/GSE137140.Rdata')
# pd: clinical
# exp
load("D:/SJTU Lessons/Rawdata1.Rdata")
# Cancerexp,Noncancerexp,Cancer,Noncancer
can=cbind(Cancer,t(Cancerexp))
noncan=cbind(Noncancer,t(Noncancerexp))
table(can$stage)
table(can$type)
# Groups need to be merged
x=can[,-c(1,4:8)]
y=noncan[,-c(1:3)]
y$stage='0'
y$type='0'
# merging & deleting
x1=x[-which(x$stage=='IV'),]
x2=x1
x2$stage[which(x1$stage %in% c('IIIA','IIIB'))]='III'
rawcombine=rbind(x2,y)

######  seperate train & test set  ######
set.seed(100)
train.can=sample(1348,1348/2)
train.noncan=sample(2178,2178/2)

can.train=x2[train.can,]
noncan.train=y[train.noncan,]
table(can.train$stage)
#table(can.train$type)
train=rbind(can.train,noncan.train)
test=rbind(x2[-train.can,],y[-train.noncan,])

######  pre-process  ######
summary(train$`hsa-let-7a-5p`)

# normalization?
# $stage$ collapse into i,ii,iii,iv ?

######  LDA & QDA  ######
library(MASS)
lda.fit = lda(stage ~ ., data=train[,-c(2:4)])
lda.fit
#plot(lda.fit)
names(lda.fit)
lda.pred = predict(lda.fit, test)
table(lda.pred$class, test$stage)
mean(lda.pred$class == test$stage)

qda.fit = qda(stage ~ ., data=train[,-c(2:4)])
qda.fit
qda.pred = predict(qda.fit, test)
table(qda.pred$class, test$stage)
mean(qda.pred$class == test$stage)


######  KNN  ######
library(class)
library(pROC)
train.X = as.matrix(train[,-c(1:4)])
test.X = as.matrix(test[,-c(1:4)])
set.seed(1)
knn.pred = knn(train.X, test.X, train$stage, k=12)
table(knn.pred, test$stage)
mean(knn.pred == test$stage)
roc=multiclass.roc(ordered(test$stage),ordered(knn.pred))
#str(knn.pred)
#str(ordered(knn.pred))

correct.rate=0
correctrate=rep(NA,50)
auc=rep(NA,50)
for (i in 1:50){
  set.seed(1)
  knn.pred = knn(train.X, test.X, train$stage, k=i)
  roc=multiclass.roc(ordered(test$stage),ordered(knn.pred))
  if (mean(knn.pred == test$stage)>correct.rate) {
    correct.rate=mean(knn.pred == test$stage)
    k <<- i
  }
  correctrate[i]=correct.rate
  auc[i]=roc$auc
}
correct.rate
k
#correct.rate=0.70345,k=92
# k=92 is not suitable
# maybe conclude that KNN perform badly, predicting only categories of 0 & IA (for KNN not suitable for high-D data).
# k=50 only predicts 0, IA & IB

plot(1:50,correctrate,type='l',col='green',xlab='K',ylab='Correct rate & AUC')
lines(1:50,auc,col='red')
legend("right", c('Correct rate', "AUC"), col=c("green", "red"), cex=1, lty=1)

max(auc)
which(auc==max(auc))
# k=12 miss 3 categories/8 all, auc=0.6466, correct.rate=0.6678

#leave one out/cv for K.


######  tree  ######
library(MASS)
library(tree)
library(randomForest)
set.seed(98)

train.tree <- train[,-c(2:4)]
colnames(train.tree)=c("stage", paste0("miRNA", 1:641))
test.tree <- test[,-c(2:4)]
colnames(test.tree)=c("stage", paste0("miRNA", 1:641))
train.tree$stage=as.factor(train.tree$stage)
test.tree$stage=as.factor(test.tree$stage)

tree.Cancer = tree(stage ~ .,train.tree)
summary(tree.Cancer)
tree.Cancer
plot(tree.Cancer)
text(tree.Cancer, pretty = 0)
pred = predict(tree.Cancer, test.tree, type = "class")
table(pred,test.tree$stage)

cv.Cancer= cv.tree(tree.Cancer, FUN = prune.misclass)
cv.Cancer= cv.tree(tree.Cancer, FUN = prune.tree)
cv.Cancer
plot(cv.Cancer$size, cv.Cancer$dev, type = "b", xlab = "Tree Size", ylab = "Deviance")
msize = cv.Cancer$size[which.min(cv.Cancer$dev)]
Cancer.pruned = prune.tree(tree.Cancer, best = msize)
Cancer.pruned
summary(Cancer.pruned)
plot(Cancer.pruned)
text(Cancer.pruned, pretty=0)

misclass.unpruned = sum(test.tree$stage != pred)
misclass.unpruned / length(pred)
pred.pruned = predict(Cancer.pruned, test, type = "class")
misclass.pruned = sum(test.tree$stage != pred.pruned)
misclass.pruned / length(pred.pruned)

# Random Forest
X.train = train.tree[,-1]
X.test = test.tree[,-1]
Y.train = train.tree[,1]
Y.test = test.tree[,1]
p = dim(train.tree)[2] - 1
p.2 = p / 2
p.sq = sqrt(p)
set.seed(114)
rf.p = randomForest(X.train, Y.train, xtest=X.test, ytest=Y.test, mtry=p, ntree=500)
rf.p.2 = randomForest(X.train, Y.train, xtest=X.test, ytest=Y.test, mtry=p.2, ntree=500)
rf.p.sq = randomForest(X.train, Y.train, xtest=X.test, ytest=Y.test, mtry=p.sq, ntree=500)
plot(1:500, rf.p$test$err.rate[,1], col="green", type="l", xlab="Number of Trees", ylab="Test MSE")
lines(1:500, rf.p.2$test$err.rate[,1], col="red", type="l")
lines(1:500, rf.p.sq$test$err.rate[,1], col="blue", type="l")
legend("topright", c("m=p", "m=p/2", "m=sqrt(p)"), col=c("green", "red", "blue"), cex=1, lty=1)
# p/2 is the best.

table(rf.p.2$predicted,test.tree$stage)
mean(rf.p.2$predicted == test.tree$stage)

# Boosting
library(gbm)
set.seed(342)
boost.Cancer = gbm(stage~., data=train.tree, n.trees=1000, shrinkage=0.01)
summary(boost.Cancer)
boost.prob = predict(boost.Cancer,test.tree, n.trees=1000, type="response")
#boost.pred = ifelse(boost.prob >0.5, 1, 0)
stage.pred=rep(NA,1767)
for (i in 1:1767){
  for (j in 1:8){
    if (boost.prob[i,j,1]==max(boost.prob[i,,1])){
      stage.pred[i]=colnames(boost.prob)[j]
    }
  }
}
stage.pred=as.factor(stage.pred)
table(stage.pred,test.tree$stage)
mean(stage.pred == test.tree$stage)

# calculate macro-average for boosting model
confu=as.matrix(table(stage.pred,test.tree$stage))
macro.ave=function(confusion.matrix,test=2){
  p=dim(confusion.matrix)[2]
  if (test==2){
    y=rep(NA,p)
    for (i in 1:p){
      y[i]=confusion.matrix[i,i]/sum(confusion.matrix[,i])
    }
    return(mean(y))
  } else if (test==1){
    y=rep(NA,p)
    for (i in 1:p){
      y[i]=confusion.matrix[i,i]/sum(confusion.matrix[i,])
    }
    return(mean(y))
  }
}
macro.ave(confu)
macro.ave(as.matrix(table(lda.pred$class, test$stage)))
macro.ave(as.matrix(table(knn.pred, test$stage)))
macro.ave(as.matrix(table(pred,test.tree$stage)))
macro.ave(as.matrix(table(rf.p.2$predicted,test.tree$stage)))

```





