---
title: "Homework09_Section 9.7 Exercises"
author: "Guanghua Qiao"
date: "2020.6.1"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Exercise 1
```{r}
x1 = -10:10
x2 = 1 + 3 * x1
plot(x1, x2, type="l", col="red")
text(c(-3), c(-20), "greater than 0", col="red")
text(c(-7), c(-10), "less than 0", col="red")
lines(x1, 1 - x1/2)
text(c(5), c(-8), "less than 0")
text(c(7), c(6), "greater than 0")
```


## Exercise 2
###(a) (b)
```{r 2b}
radius = 2
plot(NA, NA, type="n", xlim=c(-4,2), ylim=c(-1,5), asp=1, xlab="X1", ylab="X2")
symbols(c(-1), c(2), circles=c(radius), add=TRUE, inches=FALSE)
text(c(-1), c(2), "< 4")
text(c(-4), c(2), "> 4")
```
On the curve (a circle): $=4$

###(c)
$(0,0)$, $(2,2)$, $(3,8)$ are blue, $(-1,1)$ is red.

```{r 2c}
radius = 2
plot(c(0, -1, 2, 3), c(0, 1, 2, 8), col=c("blue", "red", "blue", "blue"),
     type="p", asp=1, xlab="X1", ylab="X2")
symbols(c(-1), c(2), circles=c(radius), add=TRUE, inches=FALSE)
```

###(d)
The decision boundary is a sum of quadratic terms when expanded.

$$
\begin{aligned}
(1+X_1)^2 + (2-X_2)^2 > 4
\\
1 + 2 X_1 + X_1^2 + 4 - 4 X_2 + X_2^2 > 4
\\
5 + 2 X_1 - 4 X_2 + X_1^2 + X_2^2 > 4
\end{aligned}
$$

So the decision boundary is linear in terms of $X_1$, $X^2_1$, $X_2$, and $X^2_2$.


## Exercise 3
###(a)
```{r 3a}
x1 = c(3,2,4,1,2,4,4)
x2 = c(4,2,4,4,1,3,1)
colors = c("Red", "Red", "Red", "Red", "Blue", "Blue", "Blue")
plot(x1,x2,col=colors,xlim=c(0,5),ylim=c(0,5))
```

###(b)
The maximal margin classifier has to be in between observations #2, #3 and #5, #6.

$$
\begin{aligned}
(2,2), (4,4) \\
(2,1), (4,3) \\
=> (2,1.5), (4,3.5) \\
b = (3.5 - 1.5) / (4 - 2) = 1 \\
a = 1.5 - 2 = -0.5
\end{aligned}
$$

```{r 3b}
plot(x1,x2,col=colors,xlim=c(0,5),ylim=c(0,5))
abline(-0.5, 1)
```

###(c)
Classify to $Red$ if $0.5 - X_1 + X_2 > 0$, and classify to $Blue$ otherwise.

###(d)
```{r 3d}
plot(x1,x2,col=colors,xlim=c(0,5),ylim=c(0,5))
abline(-0.5, 1)
abline(-1, 1, lty=2)
abline(0, 1, lty=2)
```

###(e)
```{r 3e}
plot(x1,x2,col=colors,xlim=c(0,5),ylim=c(0,5))
abline(-0.5, 1)
arrows(2,1,2,1.5,lwd=2)
arrows(2,2,2,1.5)
arrows(4,4,4,3.5)
arrows(4,3,4,3.5,lwd=2)
```

###(f)
A slight movement of observation #7 $(4,1)$ $Blue$ would not have an effect on the maximal margin hyperplane because its movement would be outside of the margin.

###(g)
```{r 3g}
plot(x1,x2,col=colors,xlim=c(0,5),ylim=c(0,5))
abline(-0.3, 0.9)
```
$0.3 - 0.9X_1 + X_2 = 0$ 

###(h)
```{r 3h}
plot(x1,x2,col=colors,xlim=c(0,5),ylim=c(0,5))
points(c(4.2), c(2), col=c("red"))
```


## Exercise 5
###(a)
```{r}
set.seed(421)
x1 = runif(500) - 0.5
x2 = runif(500) - 0.5
y = 1 * (x1^2 - x2^2 > 0)
```

###(b)
```{r 5b}
plot(x1[y==0], x2[y==0], col="red", xlab="X1", ylab="X2", pch="+")
points(x1[y==1], x2[y==1], col="blue", pch=4)
```


###(c)
```{r}
lm.fit = glm(y~x1+x2, family=binomial)
summary(lm.fit)
```
Both variables are insignificant for predicting $y$.


###(d)
```{r 5d}
data = data.frame(x1=x1, x2=x2, y=y)
lm.prob = predict(lm.fit, data, type="response")
lm.pred = ifelse(lm.prob > 0.52, 1, 0)
data.pos = data[lm.pred == 1, ]
data.neg = data[lm.pred == 0, ]
plot(data.pos$x1, data.pos$x2, col="blue", xlab="X1", ylab="X2", pch="+")
points(data.neg$x1, data.neg$x2, col="red", pch=4)
```
With the model in (c) and a probability threshold of 0.5, all points are classified to single class and no decision boundary can be shown. Hence we shift the probability threshold to 0.52 to show a meaningful decision boundary. This boundary is linear as seen in the figure.


###(e)
```{r}
lm.fit = glm(y~poly(x1, 2)+poly(x2, 2) + I(x1 * x2), data=data, family=binomial)
```

###(f)
```{r 5f}
lm.prob = predict(lm.fit, data, type="response")
lm.pred = ifelse(lm.prob > 0.5, 1, 0)
data.pos = data[lm.pred == 1, ]
data.neg = data[lm.pred == 0, ]
plot(data.pos$x1, data.pos$x2, col="blue", xlab="X1", ylab="X2", pch="+")
points(data.neg$x1, data.neg$x2, col="red", pch=4)
```
This non-linear decision boundary closely resembles the true decision boundary.


###(g)
```{r 5g}
library(e1071)
svm.fit = svm(as.factor(y)~x1+x2, data, kernel="linear", cost=0.1)
svm.pred = predict(svm.fit, data)
data.pos = data[svm.pred == 1, ]
data.neg = data[svm.pred == 0, ]
plot(data.pos$x1, data.pos$x2, col="blue", xlab="X1", ylab="X2", pch="+")
points(data.neg$x1, data.neg$x2, col="red", pch=4)
```
A linear kernel, even with low cost fails to find non-linear decision boundary and classifies all points to a single class.


###(h)
```{r 5h}
svm.fit = svm(as.factor(y)~x1+x2, data, gamma=1)
svm.pred = predict(svm.fit, data)
data.pos = data[svm.pred == 1, ]
data.neg = data[svm.pred == 0, ]
plot(data.pos$x1, data.pos$x2, col="blue", xlab="X1", ylab="X2", pch="+")
points(data.neg$x1, data.neg$x2, col="red", pch=4)
```
Again, the non-linear decision boundary on predicted labels closely resembles the true decision boundary.


###(i)
This experiment enforces the idea that SVMs with non-linear kernel are extremely powerful in finding non-linear boundary. Both, logistic regression with non-interactions and SVMs with linear kernels fail to find the decision boundary. Adding interaction terms to logistic regression seems to give them same power as radial-basis kernels. However, there is some manual efforts and tuning involved in picking right interaction terms. This effort can become prohibitive with large number of features. Radial basis kernels, on the other hand, only require tuning of one parameter - gamma - which can be easily done using cross-validation.



## Exercise 7
###(a)
```{r}
library(ISLR)
gas.med = median(Auto$mpg)
new.var = ifelse(Auto$mpg > gas.med, 1, 0)
Auto$mpglevel = as.factor(new.var)
```

###(b)
```{r}
library(e1071)
set.seed(3)
tune.out = tune(svm, mpglevel~., data=Auto, kernel="linear",
                ranges=list(cost=c(0.01, 0.1, 1, 5, 10, 100)))
summary(tune.out)
```
Cross-validation error is minimized for $\tt{cost}=1$.


###(c)
```{r}
set.seed(3)
tune.out = tune(svm, mpglevel~., data=Auto, kernel="polynomial",
                ranges=list(cost=c(0.1, 1, 5, 10), degree=c(2, 3, 4)))
summary(tune.out)
```
For polynomial basis kernel, the lowest cross-validation error is obtained for $\tt{cost} = 10$ and $\tt{degree} = 2$.

```{r}
set.seed(3)
tune.out = tune(svm, mpglevel~., data=Auto, kernel="radial",
                ranges=list(cost=c(0.1, 1, 5, 10), gamma=c(0.01, 0.1, 1, 5, 10, 100)))
summary(tune.out)
```
For radial basis kernel, the lowest cross-validation error is obtained for $\tt{cost} = 10$ and $\tt{gamma} = 0.01$.

###(d)
```{r 7d}
svm.linear = svm(mpglevel~., data=Auto, kernel="linear", cost=1)
svm.poly = svm(mpglevel~., data=Auto[,-c(1,9)], kernel="polynomial", cost=10, degree=2)
svm.radial = svm(mpglevel~., data=Auto[,-c(1,9)], kernel="radial", cost=10, gamma=0.01)
plotpairs = function(fit){
  for (name in names(Auto)[!(names(Auto) %in% c("mpg", "mpglevel","name"))]) {
    plot(fit, Auto, as.formula(paste("mpg~", name, sep="")))
  }
}
plotpairs(svm.linear)
plotpairs(svm.poly)
plotpairs(svm.radial)
```


## Exercise 8
###(a)
```{r}
library(ISLR)
set.seed(90)
train = sample(dim(OJ)[1], 800)
OJ.train = OJ[train, ]
OJ.test = OJ[-train, ]
```

###(b)
```{r}
library(e1071)
svm.linear = svm(Purchase~., kernel="linear", data=OJ.train, cost=0.01)
summary(svm.linear)
```
Support vector classifier creates 447 support vectors out of 800 training points. Out of these, 224 belong to level $\tt{CH}$ and remaining 223 belong to level $\tt{MM}$.


###(c)
```{r}
train.pred = predict(svm.linear, OJ.train)
table(OJ.train$Purchase, train.pred)
mean(OJ.train$Purchase!=train.pred)
test.pred = predict(svm.linear, OJ.test)
table(OJ.test$Purchase, test.pred)
mean(OJ.test$Purchase!=test.pred)
```
The training error rate is $17.1$% and the test error rate is about $14.4$%.


###(d)
```{r}
set.seed(90)
tune.out = tune(svm, Purchase~., data=OJ.train, kernel="linear",
                ranges=list(cost=10^seq(-2, 1, by=0.1)))
summary(tune.out)
```
Tuning shows that optimal cost is 3.1623

###(e)
```{r}
svm.linear = svm(Purchase~., kernel="linear", data=OJ.train, cost=tune.out$best.parameters$cost)
train.pred = predict(svm.linear, OJ.train)
table(OJ.train$Purchase, train.pred)
mean(OJ.train$Purchase!=train.pred)
test.pred = predict(svm.linear, OJ.test)
table(OJ.test$Purchase, test.pred)
mean(OJ.test$Purchase!=test.pred)
```
The training error decreases to $16.75$% but the test error slightly increases to $14.8$%.


###(f)
```{r}
set.seed(90)
svm.radial = svm(Purchase~., data=OJ.train, kernel="radial")
summary(svm.radial)
train.pred = predict(svm.radial, OJ.train)
table(OJ.train$Purchase, train.pred)
mean(OJ.train$Purchase!=train.pred)
test.pred = predict(svm.radial, OJ.test)
table(OJ.test$Purchase, test.pred)
mean(OJ.test$Purchase!=test.pred)
```
The radial basis kernel with default gamma creates 389 support vectors, out of which, 195 belong to level $\tt{CH}$ and remaining 194 belong to level $\tt{MM}$.

The classifier has a training error of $15.4$% and a test error of $14.8$%.

Then, use cross validation to find optimal gamma.

```{r}
set.seed(90)
tune.out = tune(svm, Purchase~., data=OJ.train, kernel="radial",
                ranges=list(cost=10^seq(-2, 1, by=0.25)))
summary(tune.out)
svm.radial = svm(Purchase~., data=OJ.train, kernel="radial", cost=tune.out$best.parameters$cost)
train.pred = predict(svm.radial, OJ.train)
table(OJ.train$Purchase, train.pred)
mean(OJ.train$Purchase!=train.pred)
test.pred = predict(svm.radial, OJ.test)
table(OJ.test$Purchase, test.pred)
mean(OJ.test$Purchase!=test.pred)
```
Tuning shows that optimal cost is 0.5623

New value for gamma slightly increases training error to $15.75$% and increases test error to $15.56$% which means cross-validation for gamma is not necessarily omnipotent.

###(g)
```{r}
set.seed(90)
svm.poly = svm(Purchase~., data=OJ.train, kernel="poly", degree=2)
summary(svm.poly)
train.pred = predict(svm.poly, OJ.train)
table(OJ.train$Purchase, train.pred)
mean(OJ.train$Purchase!=train.pred)
test.pred = predict(svm.poly, OJ.test)
table(OJ.test$Purchase, test.pred)
mean(OJ.test$Purchase!=test.pred)
```
Summary shows that polynomial kernel produces 463 support vectors, out of which, 236 belong to level $\tt{CH}$ and remaining 227 belong to level $\tt{MM}$.

This kernel produces a train error of $18.9$% and a test error of $18.1$% which are higher than the errors produces by both radial kernel and linear kernel.

```{r}
set.seed(90)
tune.out = tune(svm, Purchase~., data=OJ.train, kernel="poly", degree=2,
                ranges=list(cost=10^seq(-2, 1, by=0.25)))
summary(tune.out)
svm.poly = svm(Purchase~., data=OJ.train, kernel="poly", degree=2, cost=tune.out$best.parameters$cost)
train.pred = predict(svm.poly, OJ.train)
table(OJ.train$Purchase, train.pred)
mean(OJ.train$Purchase!=train.pred)
test.pred = predict(svm.poly, OJ.test)
table(OJ.test$Purchase, test.pred)
mean(OJ.test$Purchase!=test.pred)
```
Tuning shows that optimal cost is 5.6234

Tuning reduces the training error to $15.1$% and the test error to $17.4$%.


###(h)
Linear kernel and radial basis kernel have similar results and seem to be producing minimum misclassification error. They are both better than polynomial basis kernel.

Overall, radial basis kernel seems to give the best results on this data with regard to its training and test error.









