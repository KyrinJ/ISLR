---
title: "Homework08_Section 8.4 Exercises"
author: "Guanghua Qiao"
date: "2020.5.22"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Exercise 1
```{r label="1"}
par(xpd=NA)
plot(NA, NA, type="n", xlim=c(0,100), ylim=c(0,100), xlab="X", ylab="Y")
# t1: x = 60; (60, 0) (60, 100)
lines(x=c(60,60),y=c(0,100))
text(x=60, y=108, labels=c("t1"), col="red")
# t2: y = 60; (0, 60) (60, 60)
lines(x=c(0,60), y=c(60,60))
text(x=30, y=60, labels=c("t2"), col="red")
# t3: x = 80; (80,0) (80, 100)
lines(x=c(80,80),y=c(0,100))
text(x=80, y=108, labels=c("t3"), col="red")
# t4: x = 20; (20,0) (20, 60)
lines(x=c(20,20),y=c(0,60))
text(x=20, y=30, labels=c("t4"), col="red")
# t5: y=20; (80,20) (100,20)
lines(x=c(80,100),y=c(20,20))
text(x=90, y=20, labels=c("t5"), col="red")
text(x=(60+80)/2, y=50, labels=c("R1"))
text(x=30, y=80, labels=c("R2"))
text(x=90, y=60, labels=c("R3"))
text(x=90, y=10, labels=c("R4"))
text(x=40, y=30, labels=c("R5"))
text(x=10, y=30, labels=c("R6"))
```

```
        [  X<60 ] 
        |       |
    [Y<60]    [X<80]
    |   |     |    |
 [X<20] R2    R1   [Y<20]
 |    |            |    |
 R6   R5           R4   R3
```

## Exercise 2
Based on algorithm 8.2, Boosting for Regression Trees

$0.$ $\hat{f}(x) = 0, r_i = y_i$

$1.$ a) $\hat{f}^1(x) = \beta_{1_1} I(X_1 < t_1) + \beta_{0_1}$

$1.$ b) $\hat{f}(x) = \lambda\hat{f}^1(x)$

$1.$ c) $r_i = y_i - \lambda\hat{f}^1(x_i)$

For the $j$th iteration, where $b=j$:

$j.$ a) $\hat{f}^j(x) = \beta_{1_j} I(X_j < t_j) + \beta_{0_j}$

$j.$ b) $\hat{f}(x) = \lambda\hat{f}^1(X_1) + \dots + \lambda\hat{f}^j(X_j) + \dots +
\lambda\hat{f}^{p-1}(X_{p-1}) + \lambda\hat{f}^p(X_p)$

Since each iteration's fit is a distinct variable stump, there are only $p$
fits based on "$j.$ b)".

So, boosting using depth-one trees (or stumps) leads to an additive model of the form
$$f(X) = \sum_{j=1}^{p} f_j(X_j)$$


## Exercise 3
```{r label="3"}
p = seq(0, 1, .01)
gini = p * (1-p) * 2
entropy = - (p * log(p) + (1-p) * log(1-p))
class.err = 1 - pmax(p, 1-p)
plot(p,gini,type='l',xlab='p',ylab='Quantities',ylim=c(0,0.7),col='red')
lines(p,entropy,col='green')
lines(p,class.err,col='blue')
legend('topright',c('gini', 'entropy', 'class.err'),lty=1,col=c('red','green','blue'))
```

#
#
## Exercise 4
###(a)
```
              [X1 < 1]
              |      |
       [X2 < 1]      5
       |      |
[X1 < 0]      15
|      |
3      [X2<0]
       |    |
      10    0
```
###(b)
```{r label="4b"}
par(xpd=NA)
plot(NA, NA, type="n", xlim=c(-2,2), ylim=c(-3,3), xlab="X1", ylab="X2")
# X2 < 1
lines(x=c(-2,2), y=c(1,1))
# X1 < 1 with X2 < 1
lines(x=c(1,1), y=c(-3,1))
text(x=(-2+1)/2, y=-1, labels=c(-1.80))
text(x=1.5, y=-1, labels=c(0.63))
# X2 < 2 with X2 >= 1
lines(x=c(-2,2), y=c(2,2))
text(x=0, y=2.5, labels=c(2.49))
# X1 < 0 with X2<2 and X2>=1
lines(x=c(0,0), y=c(1,2))
text(x=-1, y=1.5, labels=c(-1.06))
text(x=1, y=1.5, labels=c(0.21))
```

## Exercise 5
```{r}
p = c(0.1, 0.15, 0.2, 0.2, 0.55, 0.6, 0.6, 0.65, 0.7, 0.75)
```

### Majority approach
```{r}
sum(p>0.5) > sum(p<0.5)  
```
The number of red predictions is greater than the number of green predictions.

Based on the majority vote approach, the final classification is RED.

### Average approach
```{r}
mean(p)
```
The average of the probabilities is less than the 50% threshold.

Based on the average probability, the final classification is GREEN.


## Exercise 6
### Algorithm 8.1 Building a Regression Tree
1. Use recursive binary splitting to grow a large tree on the training
data, stopping only when each terminal node has fewer than some
minimum number of observations.

2. Apply cost complexity pruning to the large tree in order to obtain a
sequence of best subtrees, as a function of $\alpha$.

3. Use K-fold cross-validation to choose $\alpha$. That is, divide the training
observations into $K$ folds. For each $k = 1, . . .,K$:
(a) Repeat Steps 1 and 2 on all but the $k$th fold of the training data.
(b) Evaluate the mean squared prediction error on the data in the
left-out $k$th fold, as a function of $\alpha$.
Average the results for each value of $\alpha$, and pick $\alpha$ to minimize the
average error.

4. Return the subtree from Step 2 that corresponds to the chosen value
of $\alpha$.


## Exercise 7
For $\tt{Boston}$ data, $p = 13$.

```{r 9a}
library(MASS)
library(randomForest)
set.seed(100)
train = sample(dim(Boston)[1], dim(Boston)[1] / 2)
X.train = Boston[train, -14]
X.test = Boston[-train, -14]
Y.train = Boston[train, 14]
Y.test = Boston[-train, 14]
p = dim(Boston)[2] - 1
p.2 = p / 2
p.sq = sqrt(p)
rf.boston.p = randomForest(X.train, Y.train, xtest=X.test, ytest=Y.test, mtry=p, ntree=500)
rf.boston.p.2 = randomForest(X.train, Y.train, xtest=X.test, ytest=Y.test, mtry=p.2, ntree=500)
rf.boston.p.sq = randomForest(X.train, Y.train, xtest=X.test, ytest=Y.test, mtry=p.sq, ntree=500)
plot(1:500, rf.boston.p$test$mse, col="green", type="l", xlab="Number of Trees", ylab="Test MSE",
     ylim=c(12, 22))
lines(1:500, rf.boston.p.2$test$mse, col="red", type="l")
lines(1:500, rf.boston.p.sq$test$mse, col="blue", type="l")
legend("topright", c("m=p", "m=p/2", "m=sqrt(p)"), col=c("green", "red", "blue"), cex=1, lty=1)
```
The plot shows that test MSE for single tree is quite high. 

Test MSE is reduced by adding more trees to the model and stabilizes around 200 to 300 hundred trees. 

Test MSE for including all variables at split is higher as compared to both using half or square-root number of variables. 

With different setting of random seeds, the test MSE for different values of $m$ varies a lot.


## Exercise 9
###(a)
```{r}
library(ISLR)
attach(OJ)
set.seed(98)
train = sample(dim(OJ)[1], 800)
OJ.train = OJ[train, ]
OJ.test = OJ[-train, ]
```

###(b)
```{r 9b}
library(tree)
oj.tree = tree(Purchase~., data=OJ.train)
summary(oj.tree)
```

The tree uses two variables: $\tt{LoyalCH}$, $\tt{PriceDiff}$ and $\tt{ListPriceDiff}$. 
Training error rate (misclassification error) for the tree is $0.1688$.
The tree has $7$ terminal nodes. 

###(c)
```{r 9c}
oj.tree
```
Let's pick terminal node labeled "10)". The splitting variable at this node is $\tt{PriceDiff}$. The splitting value of this node is $0.05$. There are $60$ points in the subtree below this node. The deviance for all points contained in region below this node is $62.72$. A "*" in the line denotes that this is a terminal node. 

The prediction at this node is $\tt{Purchase}$ = $\tt{MM}$. About $21.7$% points in this node have $\tt{CH}$ as value of $\tt{Purchase}$. Remaining $78.3$% points have $\tt{MM}$ as value of $\tt{Purchase}$.

###(d)
```{r 9d}
plot(oj.tree)
text(oj.tree, pretty=0)
```
$\tt{LoyalCH}$ is the most important variable of the tree. Top 3 nodes contain $\tt{LoyalCH}$. If $\tt{LoyalCH} < 0.276$, the tree predicts $\tt{MM}$. If $\tt{LoyalCH} > 0.754$, the tree predicts $\tt{CH}$. For intermediate values of $\tt{LoyalCH}$, the decision also depends on the value of $\tt{PriceDiff}$ and $\tt{ListPriceDiff}$.

###(e)
```{r 9e}
oj.pred = predict(oj.tree, OJ.test, type="class")
table(OJ.test$Purchase, oj.pred)
```
The test error rate is $(28+23)/(132+87+28+23)=0.1889$

###(f)
```{r 9f}
cv.oj = cv.tree(oj.tree, FUN=prune.tree)
cv.oj
```

The optimal tree size is 5.

###(g)
```{r 9g}
plot(cv.oj$size, cv.oj$dev, type="b", xlab="Tree Size", ylab="Deviance")
```

###(h)
Tree size of 5 gives the lowest cross-validation error.

###(i)
```{r 9i}
oj.pruned = prune.tree(oj.tree, best=5)
oj.pruned
plot(oj.pruned)
text(oj.pruned, pretty=0)
```

###(j)
```{r 9j}
summary(oj.pruned)
```
Training misclassification error rate of the pruned tree is $0.19$. 
While training error rate of the unpruned tree is $0.1688$.

The training error rate of the pruned tree is higher.

###(k)
```{r 9k}
pred.pruned = predict(oj.pruned, OJ.test, type="class")
misclass.pruned = sum(OJ.test$Purchase != pred.pruned)
misclass.pruned / length(pred.pruned)
pred.unpruned = predict(oj.tree, OJ.test, type="class")
misclass.unpruned = sum(OJ.test$Purchase != pred.unpruned)
misclass.unpruned / length(pred.unpruned)
```
Test error rate of the pruned tree is $0.2037$. 
While test error rate of the unpruned tree is $0.1889$.

The test error rate of the pruned tree is higher.


## Exercise 11
###(a)
```{r 11a}
library(ISLR)
train = 1:1000
Caravan$Purchase = ifelse(Caravan$Purchase == "Yes", 1, 0)
Caravan.train = Caravan[train, ]
Caravan.test = Caravan[-train, ]
```

###(b)
```{r 11b}
library(gbm)
set.seed(342)
boost.caravan = gbm(Purchase~., data=Caravan.train, n.trees=1000, shrinkage=0.01,
                    distribution="bernoulli")
summary(boost.caravan)
```
$\tt{PPERSAUT}$, $\tt{MKOOPKLA}$ and $\tt{MOPLHOOG}$ appear to be three most important variables in that order.

###(c)
```{r 11c}
boost.prob = predict(boost.caravan, Caravan.test, n.trees=1000, type="response")
boost.pred = ifelse(boost.prob >0.2, 1, 0)
table(Caravan.test$Purchase, boost.pred)
34 / (137 + 34)
```
About $20$% of people predicted to make purchase actually end up making one.

### KNN
```{r}
library(class)
knn.train=Caravan.train[ , !colnames(Caravan.train) %in% c("Purchase")]
knn.test=Caravan.test[ , !colnames(Caravan.test) %in% c("Purchase")]
knn.caravan=knn(knn.train,knn.test,Caravan.train$Purchase,k=50,prob = T)
knn.pred = ifelse(attr(knn.caravan,"prob") > 0.2, 1, 0)
table(Caravan.test$Purchase,knn.pred)

precision=0
for (i in 1:100){
  set.seed(342)
  knn.caravan=knn(knn.train,knn.test,Caravan.train$Purchase,k=i,prob=T)
  knn.pred = ifelse(attr(knn.caravan,"prob") > 0.2, 1, 0)
  if (sum(Caravan.test$Purchase==knn.pred & knn.pred==1)/sum(knn.pred==1)>precision) {
    precision=sum(Caravan.test$Purchase==knn.pred & knn.pred==1)/sum(knn.pred==1)
    k <<- i
  }
}
precision
k
```
KNN performs very poorly on this dataset when predicting people whether to make purchase if the estimated probability of purchase is greater than 20 %.

### Logistic Regression
```{r}
lm.caravan = glm(Purchase~., data=Caravan.train, family=binomial)
lm.prob = predict(lm.caravan, Caravan.test, type="response")
lm.pred = ifelse(lm.prob > 0.2, 1, 0)
table(Caravan.test$Purchase, lm.pred)
58 / (350 + 58)
```
About $14$% of people predicted to make purchase using logistic regression actually end up making one. 

This is lower than boosting. So boosting performs better.


