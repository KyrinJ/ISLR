---
title: "Project"
author: "Guanghua Qiao"
date: "2020.5.19"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#load('D:/SJTU Lessons/GSE137140.Rdata')
# pd: clinical
# exp
load("D:/SJTU Lessons/Rawdata1.Rdata")
# Cancerexp,Noncancerexp,Cancer,Noncancer
can=cbind(Cancer,t(Cancerexp))
noncan=cbind(Noncancer,t(Noncancerexp))
table(can$stage)
table(can$type)
# Groups need to be merged
x=can[,-c(1,4:8)]
y=noncan[,-c(1:3)]
y$stage='0'
y$type='0'
rawcombine=rbind(x,y)

######  seperate train & test set  ######
set.seed(100)
train.can=sample(1356,1356/2)
train.noncan=sample(2178,2178/2)

can.train=x[train.can,]
noncan.train=y[train.noncan,]
table(can.train$stage)
table(can.train$type)
train=rbind(can.train,noncan.train)
test=rbind(x[-train.can,],y[-train.noncan,])

######  pre-process  ######
summary(train$`hsa-let-7a-5p`)

# normalization?
# $stage$ collapse into i,ii,iii,iv ?

######  LDA & QDA  ######





######  KNN  ######
library(class)
library(pROC)
train.X = as.matrix(train[,-c(1:4)])
test.X = as.matrix(test[,-c(1:4)])
set.seed(1)
knn.pred = knn(train.X, test.X, train$stage, k=12)
table(knn.pred, test$stage)
mean(knn.pred == test$stage)
roc=multiclass.roc(ordered(test$stage),ordered(knn.pred))
#str(knn.pred)
#str(ordered(knn.pred))

correct.rate=0
correctrate=rep(NA,50)
auc=rep(NA,50)
for (i in 1:50){
  set.seed(1)
  knn.pred = knn(train.X, test.X, train$stage, k=i)
  roc=multiclass.roc(ordered(test$stage),ordered(knn.pred))
  if (mean(knn.pred == test$stage)>correct.rate) {
    correct.rate=mean(knn.pred == test$stage)
    k <<- i
  }
  correctrate[i]=correct.rate
  auc[i]=roc$auc
}
correct.rate
k
#correct.rate=0.70345,k=92
# k=92 is not suitable
# maybe conclude that KNN perform badly, predicting only categories of 0 & IA (for KNN not suitable for high-D data).
# k=50 only predicts 0, IA & IB

plot(1:50,correctrate,type='l',col='green',xlab='K',ylab='Correct rate & AUC')
lines(1:50,auc,col='red')
legend("right", c('Correct rate', "AUC"), col=c("green", "red"), cex=1, lty=1)

max(auc)
which(auc==max(auc))
# k=12 miss 3 categories/8 all, auc=0.6466, correct.rate=0.6678

#leave one out/cv for K.


######  tree  ######









```

## Exercise 10
(a)
```{r}
library(ISLR)
summary(Weekly)
pairs(Weekly)
cor(Weekly[,-9])
```

Volume and Year seem to have a correlation.

(d)
```{r}
attach(Weekly)
train = (Year <= 2008)
test = Weekly[!train,]
l.fit2=glm(Direction~Lag2,data=Weekly,family=binomial,subset=train)
summary(l.fit2)
predp = predict(l.fit2,test,type = 'response')
pred = rep("down", length(predp))
pred[predp>0.5] = "up"
table(pred, test$Direction)
(9+56)/104
```

Overall fraction of correct predictions for the held out data = 62.5%

(e)
```{r}
library(MASS)
lda.fit = lda(Direction ~ Lag2, data=Weekly, subset=train)
lda.pred = predict(lda.fit, test)
table(lda.pred$class, test$Direction)
mean(lda.pred$class == test$Direction)
```

Overall fraction of correct predictions for the held out data = 62.5%

(f)
```{r}
qda.fit = qda(Direction~Lag2, data=Weekly, subset=train)
qda.pred = predict(qda.fit, test)
table(qda.pred$class, test$Direction)
mean(qda.pred$class == test$Direction)
```

Overall fraction of correct predictions for the held out data = 58.65%

(g)
```{r}
library(class)
train.X = as.matrix(Lag2[train])
test.X = as.matrix(Lag2[!train])
train.Direction = Direction[train]
set.seed(1)
knn.pred = knn(train.X, test.X, train.Direction, k=1)
table(knn.pred, test$Direction)
mean(knn.pred == test$Direction)
```

Overall fraction of correct predictions for the held out data = 50%

(i)
```{r}
correct.rate=0
for (i in 1:100){
  set.seed(1)
  knn.pred = knn(train.X, test.X, train.Direction, k=i)
  if (mean(knn.pred == test$Direction)>correct.rate) {
    correct.rate=mean(knn.pred == test$Direction)
    k <<- i
  }
}
correct.rate
k

detach(Weekly)
```

When $k=4$, Overall fraction of KNN classifier correct predictions = 61.54%

Logistic regression and LDA still provide the best results on the held out data.

## Exercise 11
(a)
```{r}
#summary(Auto)
attach(Auto)
mpg01 = rep(0, length(mpg))
mpg01[mpg>median(mpg)] = 1
Auto = data.frame(Auto, mpg01)
```
(b)
```{r}
cor(Auto[,-9])
pairs(Auto)
```

mpg01 is anti-correlated with cylinders, weight, displacement and horsepower.

(c)
```{r}
train = (year %% 2 == 0)
test = !train
Auto.train = Auto[train,]
Auto.test = Auto[test,]
```

(d)
```{r}
mpg01.test = mpg01[test]
lda.fit = lda(mpg01~cylinders+weight+displacement+horsepower,
              data=Auto, subset=train)
lda.pred = predict(lda.fit, Auto.test)
mean(lda.pred$class != mpg01.test)
```

test error = 12.64%

(e)
```{r}
qda.fit = qda(mpg01~cylinders+weight+displacement+horsepower,
              data=Auto, subset=train)
qda.pred = predict(qda.fit, Auto.test)
mean(qda.pred$class != mpg01.test)
```

test error = 13.19%

(f)
```{r}
glm.fit = glm(mpg01~cylinders+weight+displacement+horsepower,
              data=Auto,
              family=binomial,
              subset=train)
glm.probs = predict(glm.fit, Auto.test, type="response")
glm.pred = rep(0, length(glm.probs))
glm.pred[glm.probs > 0.5] = 1
mean(glm.pred != mpg01.test)
```

test error = 12.09%

(g)
```{r}
train.X = cbind(cylinders, weight, displacement, horsepower)[train,]
test.X = cbind(cylinders, weight, displacement, horsepower)[test,]
mpg01.train = mpg01[train]

error.rate=1
k=0
for (i in 1:100){
  set.seed(1)
  knn.pred = knn(train.X, test.X, mpg01.train, k=i)
  if (mean(knn.pred != mpg01.test)<error.rate) {
    error.rate=mean(knn.pred != mpg01.test)
    k <<- i
  }
}
error.rate
k

detach(Auto)
```

test error = 13.74% when $K=3$

$K$ of 3 seems to perform the best on this data set.





