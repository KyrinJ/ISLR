---
title: "Project"
author: "Guanghua Qiao"
date: "2020.5.19"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#load('D:/SJTU Lessons/GSE137140.Rdata')
# pd: clinical
# exp
load("D:/SJTU Lessons/Rawdata1.Rdata")
# Cancerexp,Noncancerexp,Cancer,Noncancer
can=cbind(Cancer,t(Cancerexp))
noncan=cbind(Noncancer,t(Noncancerexp))
table(can$stage)
table(can$type)
# Groups need to be merged
x=can[,-c(1,4:8)]
y=noncan[,-c(1:3)]
y$stage='0'
y$type='0'
rawcombine=rbind(x,y)

######  seperate train & test set  ######
set.seed(100)
train.can=sample(1356,1356/2)
train.noncan=sample(2178,2178/2)

can.train=x[train.can,]
noncan.train=y[train.noncan,]
table(can.train$stage)
table(can.train$type)
train=rbind(can.train,noncan.train)

######  pre-process  ######
summary(train$`hsa-let-7a-5p`)

#normalization?




######  tree  ######





```

## Exercise 10
(a)
```{r}
library(ISLR)
summary(Weekly)
pairs(Weekly)
cor(Weekly[,-9])
```

Volume and Year seem to have a correlation.

(b)
```{r}
l.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,data=Weekly,family=binomial)
summary(l.fit)
```

Lag2 appears to be statistically significant.

(c)
```{r}
predp = predict(l.fit, type="response")
pred = rep("down", length(predp))
pred[predp>0.5] = "up"
table(pred, Weekly$Direction)
(54+557)/1089
```

Overall fraction of correct predictions = 56.1%

For obsrvations with Direction is 'Up', prediction accuracy is high, which is 557/(557+48) = 92.1%

For obsrvations with Direction is 'Down', prediction accuracy is low, which is 54/(430+54) = 11.2%

(d)
```{r}
attach(Weekly)
train = (Year <= 2008)
test = Weekly[!train,]
l.fit2=glm(Direction~Lag2,data=Weekly,family=binomial,subset=train)
summary(l.fit2)
predp = predict(l.fit2,test,type = 'response')
pred = rep("down", length(predp))
pred[predp>0.5] = "up"
table(pred, test$Direction)
(9+56)/104
```

Overall fraction of correct predictions for the held out data = 62.5%

(e)
```{r}
library(MASS)
lda.fit = lda(Direction ~ Lag2, data=Weekly, subset=train)
lda.pred = predict(lda.fit, test)
table(lda.pred$class, test$Direction)
mean(lda.pred$class == test$Direction)
```

Overall fraction of correct predictions for the held out data = 62.5%

(f)
```{r}
qda.fit = qda(Direction~Lag2, data=Weekly, subset=train)
qda.pred = predict(qda.fit, test)
table(qda.pred$class, test$Direction)
mean(qda.pred$class == test$Direction)
```

Overall fraction of correct predictions for the held out data = 58.65%

(g)
```{r}
library(class)
train.X = as.matrix(Lag2[train])
test.X = as.matrix(Lag2[!train])
train.Direction = Direction[train]
set.seed(1)
knn.pred = knn(train.X, test.X, train.Direction, k=1)
table(knn.pred, test$Direction)
mean(knn.pred == test$Direction)
```

Overall fraction of correct predictions for the held out data = 50%

(h)

Logistic regression and LDA appear to provide the best results on this data.

(i)
```{r}
correct.rate=0
for (i in 1:100){
  set.seed(1)
  knn.pred = knn(train.X, test.X, train.Direction, k=i)
  if (mean(knn.pred == test$Direction)>correct.rate) {
    correct.rate=mean(knn.pred == test$Direction)
    k <<- i
  }
}
correct.rate
k

detach(Weekly)
```

When $k=4$, Overall fraction of KNN classifier correct predictions = 61.54%

Logistic regression and LDA still provide the best results on the held out data.

## Exercise 11
(a)
```{r}
#summary(Auto)
attach(Auto)
mpg01 = rep(0, length(mpg))
mpg01[mpg>median(mpg)] = 1
Auto = data.frame(Auto, mpg01)
```
(b)
```{r}
cor(Auto[,-9])
pairs(Auto)
```

mpg01 is anti-correlated with cylinders, weight, displacement and horsepower.

(c)
```{r}
train = (year %% 2 == 0)
test = !train
Auto.train = Auto[train,]
Auto.test = Auto[test,]
```

(d)
```{r}
mpg01.test = mpg01[test]
lda.fit = lda(mpg01~cylinders+weight+displacement+horsepower,
              data=Auto, subset=train)
lda.pred = predict(lda.fit, Auto.test)
mean(lda.pred$class != mpg01.test)
```

test error = 12.64%

(e)
```{r}
qda.fit = qda(mpg01~cylinders+weight+displacement+horsepower,
              data=Auto, subset=train)
qda.pred = predict(qda.fit, Auto.test)
mean(qda.pred$class != mpg01.test)
```

test error = 13.19%

(f)
```{r}
glm.fit = glm(mpg01~cylinders+weight+displacement+horsepower,
              data=Auto,
              family=binomial,
              subset=train)
glm.probs = predict(glm.fit, Auto.test, type="response")
glm.pred = rep(0, length(glm.probs))
glm.pred[glm.probs > 0.5] = 1
mean(glm.pred != mpg01.test)
```

test error = 12.09%

(g)
```{r}
train.X = cbind(cylinders, weight, displacement, horsepower)[train,]
test.X = cbind(cylinders, weight, displacement, horsepower)[test,]
mpg01.train = mpg01[train]

error.rate=1
k=0
for (i in 1:100){
  set.seed(1)
  knn.pred = knn(train.X, test.X, mpg01.train, k=i)
  if (mean(knn.pred != mpg01.test)<error.rate) {
    error.rate=mean(knn.pred != mpg01.test)
    k <<- i
  }
}
error.rate
k

detach(Auto)
```

test error = 13.74% when $K=3$

$K$ of 3 seems to perform the best on this data set.


## Exercise 12
(a)
```{r}
Power = function(){
  2^3
}
print(Power())
```
(b)
```{r}
Power2 = function(x,a){
  x^a
}
Power2(3,8)
```

(c)
```{r}
Power2(10,3)
Power2(8,17)
Power2(131,3)
```
(d)
```{r}
Power3=function(x,a){
  result=x^a
  return(result)
}
```
(e)
```{r 12e}
x=1:10
par(mfrow=c(1,3))
plot(x, Power3(x, 2),  log="x", ylab="y = x^2", xlab="Log of x",
     main="x^2 versus Log of x")
plot(x, Power3(x, 2),  log="y", ylab="Log of y = x^2", xlab="x",
     main="Log of x^2 versus x")
plot(x, Power3(x, 2),  log="xy", ylab="Log of y = x^2", xlab="Log of x",
     main="Log of x^2 versus Log of x")
```
(f)
```{r 12f}
PlotPower = function(x,a){
  plot(x,Power3(x,a))
}
PlotPower(1:10,3)
```




