---
title: "Homework04_Section 4.7 Exercises"
author: "Guanghua Qiao"
date: "2020.4.11"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Exercise 2
###(4-12)
$$
p_k(x) = \frac {\pi_k
                \frac {1} {\sqrt{2 \pi} \sigma}
                \exp(- \frac {1} {2 \sigma^2} (x - \mu_k)^2)
               }
               {\sum {
                \pi_l
                \frac {1} {\sqrt{2 \pi} \sigma}
                \exp(- \frac {1} {2 \sigma^2} (x - \mu_l)^2)
               }}
$$

###(4-13)

$$
\delta_k(x) = x \frac {\mu_k} {\sigma^2} - \frac {\mu_k^2} {2 \sigma^2}
              + \log(\pi_k)
$$        

###Proof: 

Suppose that $\delta_k(x) \geq \delta_i(x)$,

$$
x \frac {\mu_k} {\sigma^2} - \frac {\mu_k^2} {2 \sigma^2} + \log(\pi_k)
\geq
x \frac {\mu_i} {\sigma^2} - \frac {\mu_i^2} {2 \sigma^2} + \log(\pi_i)
$$

Exponentiation,

$$
\pi_k \exp (x \frac {\mu_k} {\sigma^2} - \frac {\mu_k^2} {2 \sigma^2})
\geq
\pi_i \exp (x \frac {\mu_i} {\sigma^2} - \frac {\mu_i^2} {2 \sigma^2})
$$
Multipy this inequality by the positive constant
$$
c = \frac {
                \frac {1} {\sqrt{2 \pi} \sigma}
                \exp(- \frac {1} {2 \sigma^2} x^2)
               }
               {\sum {
                \pi_l
                \frac {1} {\sqrt{2 \pi} \sigma}
                \exp(- \frac {1} {2 \sigma^2} (x - \mu_l)^2)
               }}
$$
then,

$$
\frac {\pi_k
                \frac {1} {\sqrt{2 \pi} \sigma}
                \exp(- \frac {1} {2 \sigma^2} (x - \mu_k)^2)
               }
               {\sum {
                \pi_l
                \frac {1} {\sqrt{2 \pi} \sigma}
                \exp(- \frac {1} {2 \sigma^2} (x - \mu_l)^2)
               }}
\geq
\frac {\pi_i
                \frac {1} {\sqrt{2 \pi} \sigma}
                \exp(- \frac {1} {2 \sigma^2} (x - \mu_i)^2)
               }
               {\sum {
                \pi_l
                \frac {1} {\sqrt{2 \pi} \sigma}
                \exp(- \frac {1} {2 \sigma^2} (x - \mu_l)^2)
               }}
$$

So, maximizing $\delta_k(x)$ is equivalent to maximizing $p_k(x)$.



## Exercise 4
(a) 10%, when $x\geq 0.05$ and $x\leq 0.95$

(b) 1% (when $X$ is not near the border)

(c) $10^{-100}=10^{-98}$%

(d) When $p$ increases linear, observations become far from one another, so training observations near any given test observation decrease exponentially and fraction of the available observations we use to make prediction decreases exponentially.

(e) 

When $p=1$, length of each side of the hypercube is 10%.

When $p=2$, length of each side of the hypercube is $\sqrt{0.1}=0.316$.

When $p=100$, length of each side of the hypercube is $0.1^{\frac{1}{100}}=0.977$.

When $p$ is large, it's not feasible to use KNN or other local approaches that perform prediction using only observations that are near the test observation.

## Exercise 5

(a)

QDA perform better on the training set.

LDA perform better on the test set.

(b)

QDA perform better both on the training set and the test set.

(c)

Improve. Because a more flexibile method will fit better when there are more samples and variance is offset by the larger sample sizes.

(d)

False. When the Bayes decision boundary is linear, QDA will overfit the data, leading to a higher test error rate than LDA.


## Exercise 7
$$
p_k(x) = \frac {\pi_k
                \frac {1} {\sqrt{2 \pi} \sigma}
                \exp(- \frac {1} {2 \sigma^2} (x - \mu_k)^2)
               }
               {\sum {
                \pi_l
                \frac {1} {\sqrt{2 \pi} \sigma}
                \exp(- \frac {1} {2 \sigma^2} (x - \mu_l)^2)
               }}
$$
$$
\begin{aligned}
p_{yes}(x) &= \frac {\pi_{yes}
                \exp(- \frac {1} {2 \sigma^2}  (x - \mu_{yes})^2)
               }
               {\sum {
                \pi_l
                \exp(- \frac {1} {2 \sigma^2} (x - \mu_l)^2)
               }}
\\
&= \frac {\pi_{yes} \exp(- \frac {1} {2 \sigma^2}  (x - \mu_{yes})^2)}
               {
                \pi_{yes} \exp(- \frac {1} {2 \sigma^2} (x - \mu_{yes})^2) +
                \pi_{no}  \exp(- \frac {1} {2 \sigma^2}  (x - \mu_{no})^2)
               }
\\
&= \frac {0.8 \exp(- \frac {1} {2 * 36}  (x - 10)^2)}
               {
                0.8 \exp(- \frac {1} {2 * 36} (x - 10)^2) +
                0.2 \exp(- \frac {1} {2 * 36}  x^2)
               }
\end{aligned}
$$
$$
p_{yes}(4)  = \frac {0.8 \exp(- \frac {1} {2 * 36}  (4 - 10)^2)}
               {
                0.8 \exp(- \frac {1} {2 * 36} (4 - 10)^2) +
                0.2 \exp(- \frac {1} {2 * 36}  4^2)
               }
        = 75.19\%
$$

## Exercise 9
(a)
$$
odds=\frac {p(X)} {1 - p(X)} = 0.37
\\
p(X) = \frac {0.37} {1.37} = 27\%
$$

(b)
$$
odds=\frac {p(X)} {1 - p(X)}=\frac{0.16}{1-0.16}=0.1905
$$

## Exercise 10
(a)
```{r}
library(ISLR)
summary(Weekly)
pairs(Weekly)
cor(Weekly[,-9])
```

Volume and Year seem to have a correlation.

(b)
```{r}
l.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,data=Weekly,family=binomial)
summary(l.fit)
```

Lag2 appears to be statistically significant.

(c)
```{r}
predp = predict(l.fit, type="response")
pred = rep("down", length(predp))
pred[predp>0.5] = "up"
table(pred, Weekly$Direction)
(54+557)/1089
```

Overall fraction of correct predictions = 56.1%

For obsrvations with Direction is 'Up', prediction accuracy is high, which is 557/(557+48) = 92.1%

For obsrvations with Direction is 'Down', prediction accuracy is low, which is 54/(430+54) = 11.2%

(d)
```{r}
attach(Weekly)
train = (Year <= 2008)
test = Weekly[!train,]
l.fit2=glm(Direction~Lag2,data=Weekly,family=binomial,subset=train)
summary(l.fit2)
predp = predict(l.fit2,test,type = 'response')
pred = rep("down", length(predp))
pred[predp>0.5] = "up"
table(pred, test$Direction)
(9+56)/104
```

Overall fraction of correct predictions for the held out data = 62.5%

(e)
```{r}
library(MASS)
lda.fit = lda(Direction ~ Lag2, data=Weekly, subset=train)
lda.pred = predict(lda.fit, test)
table(lda.pred$class, test$Direction)
mean(lda.pred$class == test$Direction)
```

Overall fraction of correct predictions for the held out data = 62.5%

(f)
```{r}
qda.fit = qda(Direction~Lag2, data=Weekly, subset=train)
qda.pred = predict(qda.fit, test)
table(qda.pred$class, test$Direction)
mean(qda.pred$class == test$Direction)
```

Overall fraction of correct predictions for the held out data = 58.65%

(g)
```{r}
library(class)
train.X = as.matrix(Lag2[train])
test.X = as.matrix(Lag2[!train])
train.Direction = Direction[train]
set.seed(1)
knn.pred = knn(train.X, test.X, train.Direction, k=1)
table(knn.pred, test$Direction)
mean(knn.pred == test$Direction)
```

Overall fraction of correct predictions for the held out data = 50%

(h)

Logistic regression and LDA appear to provide the best results on this data.

(i)
```{r}
correct.rate=0
for (i in 1:100){
  set.seed(1)
  knn.pred = knn(train.X, test.X, train.Direction, k=i)
  if (mean(knn.pred == test$Direction)>correct.rate) {
    correct.rate=mean(knn.pred == test$Direction)
    k <<- i
  }
}
correct.rate
k

detach(Weekly)
```

When $k=4$, Overall fraction of KNN classifier correct predictions = 61.54%

Logistic regression and LDA still provide the best results on the held out data.

## Exercise 11
(a)
```{r}
#summary(Auto)
attach(Auto)
mpg01 = rep(0, length(mpg))
mpg01[mpg>median(mpg)] = 1
Auto = data.frame(Auto, mpg01)
```
(b)
```{r}
cor(Auto[,-9])
pairs(Auto)
```

mpg01 is anti-correlated with cylinders, weight, displacement and horsepower.

(c)
```{r}
train = (year %% 2 == 0)
test = !train
Auto.train = Auto[train,]
Auto.test = Auto[test,]
```

(d)
```{r}
mpg01.test = mpg01[test]
lda.fit = lda(mpg01~cylinders+weight+displacement+horsepower,
              data=Auto, subset=train)
lda.pred = predict(lda.fit, Auto.test)
mean(lda.pred$class != mpg01.test)
```

test error = 12.64%

(e)
```{r}
qda.fit = qda(mpg01~cylinders+weight+displacement+horsepower,
              data=Auto, subset=train)
qda.pred = predict(qda.fit, Auto.test)
mean(qda.pred$class != mpg01.test)
```

test error = 13.19%

(f)
```{r}
glm.fit = glm(mpg01~cylinders+weight+displacement+horsepower,
              data=Auto,
              family=binomial,
              subset=train)
glm.probs = predict(glm.fit, Auto.test, type="response")
glm.pred = rep(0, length(glm.probs))
glm.pred[glm.probs > 0.5] = 1
mean(glm.pred != mpg01.test)
```

test error = 12.09%

(g)
```{r}
train.X = cbind(cylinders, weight, displacement, horsepower)[train,]
test.X = cbind(cylinders, weight, displacement, horsepower)[test,]
mpg01.train = mpg01[train]

error.rate=1
k=0
for (i in 1:100){
  set.seed(1)
  knn.pred = knn(train.X, test.X, mpg01.train, k=i)
  if (mean(knn.pred != mpg01.test)<error.rate) {
    error.rate=mean(knn.pred != mpg01.test)
    k <<- i
  }
}
error.rate
k

detach(Auto)
```

test error = 13.74% when $K=3$

$K$ of 3 seems to perform the best on this data set.


## Exercise 12
(a)
```{r}
Power = function(){
  2^3
}
print(Power())
```
(b)
```{r}
Power2 = function(x,a){
  x^a
}
Power2(3,8)
```

(c)
```{r}
Power2(10,3)
Power2(8,17)
Power2(131,3)
```
(d)
```{r}
Power3=function(x,a){
  result=x^a
  return(result)
}
```
(e)
```{r 12e}
x=1:10
par(mfrow=c(1,3))
plot(x, Power3(x, 2),  log="x", ylab="y = x^2", xlab="Log of x",
     main="x^2 versus Log of x")
plot(x, Power3(x, 2),  log="y", ylab="Log of y = x^2", xlab="x",
     main="Log of x^2 versus x")
plot(x, Power3(x, 2),  log="xy", ylab="Log of y = x^2", xlab="Log of x",
     main="Log of x^2 versus Log of x")
```
(f)
```{r 12f}
PlotPower = function(x,a){
  plot(x,Power3(x,a))
}
PlotPower(1:10,3)
```




