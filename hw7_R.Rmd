---
title: "Homework07_Section 7.9 Exercises"
author: "Guanghua Qiao"
date: "2020.5.12"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Exercise 1
###(a)
For all $x \le \xi$,

$a_1 = \beta_0$, 

$b_1 = \beta_1$, 

$c_1 = \beta_2$, 

$d_1 = \beta_3$.

###(b)
For all $x > \xi$,
$$
\begin{aligned}
f(x)
&=\beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \beta_4 (x - \xi)^3
\\
&= \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \beta_4 (x^3 - 3 x^2 \xi + 3 x \xi^2 - \xi^3)
\\
&= (\beta_0 - \beta_4 \xi^3) + (\beta_1 + 3 \beta_4 \xi^2) x + (\beta_2 - 3 \beta_4 \xi) x^2 + (\beta_3 + \beta_4) x^3
\end{aligned}
$$

Thus, 

$a_2 = \beta_0 - \beta_4 \xi^3$, 

$b_2 = \beta_1 + 3 \beta_4 \xi^2$, 

$c_2 = \beta_2 - 3 \beta_4 \xi$, 

$d_2 = \beta_3 + \beta_4$.

###(c)
$$
f_1(\xi) = \beta_0 + \beta_1 \xi + \beta_2 \xi^2 + \beta_3 \xi^3
$$
$$
\begin{aligned}
f_2(\xi) 
&= (\beta_0 - \beta_4 \xi^3) + (\beta_1 + 3 \beta_4 \xi^2) \xi + (\beta_2 - 3 \beta_4 \xi) \xi^2 + (\beta_3 + \beta_4) \xi^3
\\
&= \beta_0 - \beta_4 \xi^3 + \beta_1 \xi + 3 \beta_4 \xi^3 + \beta_2 \xi^2 - 3 \beta_4 \xi^3 + \beta_3 \xi^3 + \beta_4 \xi^3
\\
&= \beta_0 + \beta_1 \xi + \beta_2 \xi^2 + 3 \beta_4 \xi^3 - 3 \beta_4 \xi^3 + \beta_3 \xi^3 + \beta_4 \xi^3 - \beta_4 \xi^3
\\
&= \beta_0 + \beta_1 \xi + \beta_2 \xi^2 + \beta_3 \xi^3
\end{aligned}
$$
Thus, 
$f_1(\xi)=f_2(\xi)$

###(d)
$$
f_1'(\xi) = \beta_1 + 2 \beta_2 \xi + 3 \beta_3 \xi^2
$$
$$
\begin{aligned}
f_2'(\xi) &= \beta_1 + 3 \beta_4 \xi^2 + 2 (\beta_2 - 3 \beta_4 \xi) \xi + 3 (\beta_3 + \beta_4) \xi^2
\\
&= \beta_1 + 3 \beta_4 \xi^2 + 2 \beta_2 \xi - 6 \beta_4 \xi^2 + 3 \beta_3 \xi^2 + 3 \beta_4 \xi^2
\\
&= \beta_1 + 2 \beta_2 \xi + 3 \beta_3 \xi^2 + 3 \beta_4 \xi^2 + 3 \beta_4 \xi^2 - 6 \beta_4 \xi^2 
\\
&= \beta_1 + 2 \beta_2 \xi + 3 \beta_3 \xi^2
\end{aligned}
$$
Thus,
$f_1'(\xi)=f_2'(\xi)$

###(e)
$$
f_1''(x) = 2 c_1 + 6 d_1 x , f_2''(x) = 2 c_2 + 6 d_2 x
$$
$$
f_1''(\xi) = 2 \beta_2 + 6 \beta_3 \xi
$$
$$
\begin{aligned}
f_2''(\xi) &= 2 (\beta_2 - 3 \beta_4 \xi) + 6 (\beta_3 + \beta_4) \xi
\\
&= 2 \beta_2 + 6 \beta_3 \xi
\end{aligned}
$$
Thus,
$f_1''(\xi)=f_2''(\xi)$

## Exercise 2
###(a)
$g(x) = k$ because RSS term is ignored and $g(x) = k$ would minimize the area
under the curve of $g^{(0)}$.

###(b)
$g(x) = \alpha  x^2$. $g(x)$ would be quadratic to minimize the area under the curve
of its first derivative.

###(c)
$g(x) = \alpha  x^3$. $g(x)$ would be cubic to minimize the area under the curve
of its second derivative.

###(d)
$g(x) = \alpha  x^4$. $g(x)$ would be quartic to minimize the area under the curve
of its third derivative.

###(e)
The penalty term is ignored. This is the formula for linear regression,
to choose $g$ based on minimizing RSS.


## Exercise 3
```{r}
x = seq(-2,2,0.01)
y = 1 + x - 2 * (x-1)^2 * I(x>1)
plot(x, y)
```

## Exercise 4
```{r}
x = seq(-2,2,0.05)
f=function(x){
  y=rep(NA,length(x))
  for (i in 1:length(x)){
    if (x[i]>=0 & x[i]<1) y[i]=1+1 
    else if (x[i]>=1 & x[i]<=2) y[i]=3-x[i]
    else if (x[i]>=3 & x[i]<=4) y[i]=1+3*(x[i]-3)
    else if (x[i]>4 & x[i]<=5) y[i]=1+3
    else y[i]=1
  }
  return(y)
}
y=f(x)
plot(x,y)
```

## Exercise 5
###(a)
$\hat{g_2}$ is expected to have the smaller training RSS because it will be a 
higher order polynomial due to the order of the derivative penalty function.

###(b)
$\hat{g_1}$ is expected to have the smaller test RSS because $\hat{g_2}$, being a 
higher order polynomial, could overfit.

###(c)
For $\lambda = 0$, $\hat{g_1} = \hat{g_2}$.

So they will have the same training and test RSS.


## Exercise 7
```{r}
library(ISLR)
set.seed(1)
```

```{r 7.1,fig.width=16}
summary(Wage$maritl)
summary(Wage$jobclass)
par(mfrow=c(1,2))
plot(Wage$maritl, Wage$wage)
plot(Wage$jobclass, Wage$wage)
```
It appears a married couple makes more money on average than other groups. 

It also appears that Informational jobs are higher-wage than Industrial jobs on average.

### Polynomial Regression and Step functions
```{r 7.2}
fit = lm(wage~maritl, data=Wage)
deviance(fit)
fit = lm(wage~jobclass, data=Wage)
deviance(fit)
fit = lm(wage~maritl+jobclass, data=Wage)
deviance(fit)
```

### Splines
We cannot fit splines to categorical variables (factors). 

### GAMs
```{r}
options(warn=-1)
library(gam)
fit1 = gam(wage~maritl+jobclass+s(age,4), data=Wage)
deviance(fit1)
fit2 = gam(wage~maritl+jobclass+s(age,5), data=Wage)
deviance(fit2)
fit3 = gam(wage~maritl+jobclass+s(age,5)+education, data=Wage)
deviance(fit3)

gam.m1=gam(wage~s(age ,5) +education ,data=Wage)
deviance(gam.m1)

gam=gam(wage~s(age ,5),data=Wage)
deviance(gam)

anova(gam,fit2,fit3)
anova(gam.m1,fit3)
```

Marital status ("maritl") and job class do add statistically significant improvements to the previously discussed models in R Lab (Chapter7.8).


## Exercise 9
###(a)
```{r}
library(MASS)
set.seed(1)
attach(Boston)
lm.fit = lm(nox~poly(dis, 3), data=Boston)
summary(lm.fit)
dislim = range(dis)
dis.grid = seq(from=dislim[1], to=dislim[2], by=0.1)
lm.pred = predict(lm.fit, list(dis=dis.grid))
plot(nox~dis, data=Boston, col="darkgrey")
lines(dis.grid, lm.pred, col="red", lwd=2)
```
The regression output shows that all polynomial terms are significant when predicting `nox` using `dis`. 

###(b)
```{r}
RSS = rep(NA, 10)
for (i in 1:10) {
  lm.fit = lm(nox~poly(dis, i), data=Boston)
  RSS[i] = sum(lm.fit$residuals^2)
}
RSS
```
Train RSS monotonically decreases with degree of polynomial. 

###(c)
### 10-fold cross validation
```{r 9c}
library(boot)
all.deltas = rep(NA, 10)
for (i in 1:10) {
  glm.fit = glm(nox~poly(dis, i), data=Boston)
  all.deltas[i] = cv.glm(Boston, glm.fit, K=10)$delta[2]
}
all.deltas
plot(1:10, all.deltas, xlab="Polynomial degree", ylab="CV error", type="l", pch=20, lwd=2)
```
Polynomial degree of 4 has the lowest cross validation error, so 4 is selected as the best polynomial degree.

###(d)
```{r 9d}
summary(Boston$dis)
```
$dis$ has limits of about 1 and 13 respectively. I split this range in roughly equal 4 intervals and establish knots at $[4, 7, 10]$.
```{r}
library(splines)
sp.fit = lm(nox~bs(dis, df=4, knots=c(4, 7, 10)), data=Boston)
summary(sp.fit)
sp.pred = predict(sp.fit, list(dis=dis.grid))
plot(nox~dis, data=Boston, col="darkgrey")
lines(dis.grid, sp.pred, col="red", lwd=2)
```
All terms in spline fit are significant. Plot shows that the spline fits data well except at the extreme values of $dis$ (especially $dis > 10$) for lack of data points. 

###(e)
Fit regression splines with dfs between 3 and 16. 
```{r}
all.cv = rep(NA, 16)
for (i in 3:16) {
  lm.fit = lm(nox~bs(dis, df=i), data=Boston)
  all.cv[i] = sum(lm.fit$residuals^2)
}
all.cv[-c(1, 2)]
```
Train RSS decreases till df=14 except at df=9 and then slightly increases for df=15 and df=16.

###(f)  10-fold cross validation
Try all integer values of df between 3 and 16.
```{r 9f}
all.cv = rep(NA, 16)
set.seed(1)
for (i in 3:16) {
  lm.fit = glm(nox~bs(dis, df=i), data=Boston)
  all.cv[i] = cv.glm(Boston, lm.fit, K=10)$delta[2]
}
plot(3:16, all.cv[-c(1,2)], lwd=2, type="l", xlab="df", ylab="CV error")
all.cv[-c(1,2)]
```
CV error attains minimum at df=12, so $12$ is selected as the optimal degrees of freedom.


## Exercise 11
###(a)

###(b)


###(c)


###(d)


###(e)


###(f)


###(g)




