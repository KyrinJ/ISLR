---
title: "Homework07_Section 7.9 Exercises"
author: "Guanghua Qiao"
date: "2020.5.12"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Exercise 1
###(a)
For all $x \le \xi$,

$a_1 = \beta_0$, 

$b_1 = \beta_1$, 

$c_1 = \beta_2$, 

$d_1 = \beta_3$.

###(b)
For all $x > \xi$,
$$
\begin{aligned}
f(x)
&=\beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \beta_4 (x - \xi)^3
\\
&= \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \beta_4 (x^3 - 3 x^2 \xi + 3 x \xi^2 - \xi^3)
\\
&= (\beta_0 - \beta_4 \xi^3) + (\beta_1 + 3 \beta_4 \xi^2) x + (\beta_2 - 3 \beta_4 \xi) x^2 + (\beta_3 + \beta_4) x^3
\end{aligned}
$$

Thus, 

$a_2 = \beta_0 - \beta_4 \xi^3$, 

$b_2 = \beta_1 + 3 \beta_4 \xi^2$, 

$c_2 = \beta_2 - 3 \beta_4 \xi$, 

$d_2 = \beta_3 + \beta_4$.

###(c)
$$
f_1(\xi) = \beta_0 + \beta_1 \xi + \beta_2 \xi^2 + \beta_3 \xi^3
$$
$$
\begin{aligned}
f_2(\xi) 
&= (\beta_0 - \beta_4 \xi^3) + (\beta_1 + 3 \beta_4 \xi^2) \xi + (\beta_2 - 3 \beta_4 \xi) \xi^2 + (\beta_3 + \beta_4) \xi^3
\\
&= \beta_0 - \beta_4 \xi^3 + \beta_1 \xi + 3 \beta_4 \xi^3 + \beta_2 \xi^2 - 3 \beta_4 \xi^3 + \beta_3 \xi^3 + \beta_4 \xi^3
\\
&= \beta_0 + \beta_1 \xi + \beta_2 \xi^2 + 3 \beta_4 \xi^3 - 3 \beta_4 \xi^3 + \beta_3 \xi^3 + \beta_4 \xi^3 - \beta_4 \xi^3
\\
&= \beta_0 + \beta_1 \xi + \beta_2 \xi^2 + \beta_3 \xi^3
\end{aligned}
$$
Thus, 
$f_1(\xi)=f_2(\xi)$

###(d)
$$
f_1'(\xi) = \beta_1 + 2 \beta_2 \xi + 3 \beta_3 \xi^2
$$
$$
\begin{aligned}
f_2'(\xi) &= \beta_1 + 3 \beta_4 \xi^2 + 2 (\beta_2 - 3 \beta_4 \xi) \xi + 3 (\beta_3 + \beta_4) \xi^2
\\
&= \beta_1 + 3 \beta_4 \xi^2 + 2 \beta_2 \xi - 6 \beta_4 \xi^2 + 3 \beta_3 \xi^2 + 3 \beta_4 \xi^2
\\
&= \beta_1 + 2 \beta_2 \xi + 3 \beta_3 \xi^2 + 3 \beta_4 \xi^2 + 3 \beta_4 \xi^2 - 6 \beta_4 \xi^2 
\\
&= \beta_1 + 2 \beta_2 \xi + 3 \beta_3 \xi^2
\end{aligned}
$$
Thus,
$f_1'(\xi)=f_2'(\xi)$

###(e)
$$
f_1''(x) = 2 c_1 + 6 d_1 x , f_2''(x) = 2 c_2 + 6 d_2 x
$$
$$
f_1''(\xi) = 2 \beta_2 + 6 \beta_3 \xi
$$
$$
\begin{aligned}
f_2''(\xi) &= 2 (\beta_2 - 3 \beta_4 \xi) + 6 (\beta_3 + \beta_4) \xi
\\
&= 2 \beta_2 + 6 \beta_3 \xi
\end{aligned}
$$
Thus,
$f_1''(\xi)=f_2''(\xi)$

## Exercise 2
###(a)
$g(x) = k$ because RSS term is ignored and $g(x) = k$ would minimize the area
under the curve of $g^{(0)}$.

###(b)
$g(x) = \alpha  x^2$. $g(x)$ would be quadratic to minimize the area under the curve
of its first derivative.

###(c)
$g(x) = \alpha  x^3$. $g(x)$ would be cubic to minimize the area under the curve
of its second derivative.

###(d)
$g(x) = \alpha  x^4$. $g(x)$ would be quartic to minimize the area under the curve
of its third derivative.

###(e)
The penalty term is ignored. This is the formula for linear regression,
to choose $g$ based on minimizing RSS.


## Exercise 3
```{r}
x = seq(-2,2,0.01)
y = 1 + x - 2 * (x-1)^2 * I(x>1)
plot(x, y)
```

## Exercise 4
```{r}
x = seq(-2,2,0.05)
f=function(x){
  y=rep(NA,length(x))
  for (i in 1:length(x)){
    if (x[i]>=0 & x[i]<1) y[i]=1+1 
    else if (x[i]>=1 & x[i]<=2) y[i]=3-x[i]
    else if (x[i]>=3 & x[i]<=4) y[i]=1+3*(x[i]-3)
    else if (x[i]>4 & x[i]<=5) y[i]=1+3
    else y[i]=1
  }
  return(y)
}
y=f(x)
plot(x,y)
```

## Exercise 5
###(a)
$\hat{g_2}$ is expected to have the smaller training RSS because it will be a 
higher order polynomial due to the order of the derivative penalty function.

###(b)
$\hat{g_1}$ is expected to have the smaller test RSS because $\hat{g_2}$, being a 
higher order polynomial, could overfit.

###(c)
For $\lambda = 0$, $\hat{g_1} = \hat{g_2}$.

So they will have the same training and test RSS.


## Exercise 7
```{r}
library(ISLR)
set.seed(1)
```

```{r 7.1,fig.width=16}
summary(Wage$maritl)
summary(Wage$jobclass)
par(mfrow=c(1,2))
plot(Wage$maritl, Wage$wage)
plot(Wage$jobclass, Wage$wage)
```
It appears a married couple makes more money on average than other groups. 

It also appears that Informational jobs are higher-wage than Industrial jobs on average.

## Polynomial Regression and Step functions
```{r 7.2}
fit = lm(wage~maritl, data=Wage)
deviance(fit)
fit = lm(wage~jobclass, data=Wage)
deviance(fit)
fit = lm(wage~maritl+jobclass, data=Wage)
deviance(fit)
```

## Splines
Unable to fit splines on categorical variables.

## GAMs
```{r}
library(gam)
fit = gam(wage~maritl+jobclass+s(age,4), data=Wage)
deviance(fit)
```

We cannot fit splines to categorical variables (factors). 

`maritl` and `jobclass` do add statistically significant
improvements to the previously discussed models.



## Exercise 9
###(a)


###(b)


###(c)


###(d)


###(e)


###(f)



## Exercise 11
###(a)

###(b)


###(c)


###(d)


###(e)


###(f)


###(g)




